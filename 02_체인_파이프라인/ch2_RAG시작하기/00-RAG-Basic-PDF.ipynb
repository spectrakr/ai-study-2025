{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b82288e",
   "metadata": {},
   "source": [
    "# RAG 기본 구조 이해하기\n",
    "\n",
    "## 1. 사전작업(Pre-processing) - 1~4 단계\n",
    "\n",
    "![rag-1.png](./assets/rag-1.png)\n",
    "\n",
    "![rag-1-graphic](./assets/rag-graphic-1.png)\n",
    "\n",
    "사전 작업 단계에서는 데이터 소스를 Vector DB (저장소) 에 문서를 로드-분할-임베딩-저장 하는 4단계를 진행합니다.\n",
    "\n",
    "- 1단계 문서로드(Document Load): 문서 내용을 불러옵니다.\n",
    "- 2단계 분할(Text Split): 문서를 특정 기준(Chunk) 으로 분할합니다.\n",
    "- 3단계 임베딩(Embedding): 분할된(Chunk) 를 임베딩하여 저장합니다.\n",
    "- 4단계 벡터DB 저장: 임베딩된 Chunk 를 DB에 저장합니다.\n",
    "\n",
    "## 2. RAG 수행(RunTime) - 5~8 단계\n",
    "\n",
    "![rag-2.png](./assets/rag-2.png)\n",
    "\n",
    "![](./assets/rag-graphic-2.png)\n",
    "\n",
    "- 5단계 검색기(Retriever): 쿼리(Query) 를 바탕으로 DB에서 검색하여 결과를 가져오기 위하여 리트리버를 정의합니다. 리트리버는 검색 알고리즘이며(Dense, Sparse) 리트리버로 나뉘게 됩니다. Dense: 유사도 기반 검색, Sparse: 키워드 기반 검색\n",
    "- 6단계 프롬프트: RAG 를 수행하기 위한 프롬프트를 생성합니다. 프롬프트의 context 에는 문서에서 검색된 내용이 입력됩니다. 프롬프트 엔지니어링을 통하여 답변의 형식을 지정할 수 있습니다.\n",
    "- 7단계 LLM: 모델을 정의합니다.(GPT-3.5, GPT-4, Claude, etc..)\n",
    "- 8단계 Chain: 프롬프트 - LLM - 출력 에 이르는 체인을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf05522",
   "metadata": {},
   "source": [
    "## 실습에 활용한 문서\n",
    "\n",
    " '01. 온보딩 프로세스 - 기반기술.pdf\n",
    "\n",
    "B팀에서 작성한 문서를 pdf로 변환한 파일입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c423a8",
   "metadata": {},
   "source": [
    "## 환경설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224fd32",
   "metadata": {},
   "source": [
    "API KEY 를 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "418ab505",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:57:17.667235Z",
     "start_time": "2025-02-26T08:57:17.659760Z"
    }
   },
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "9b0d050a",
   "metadata": {},
   "source": [
    "## RAG 기본 파이프라인(1~8단계)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3d1b0fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:55:16.252607Z",
     "start_time": "2025-02-26T08:55:11.455263Z"
    }
   },
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "48e783c4",
   "metadata": {},
   "source": [
    "아래는 기본적인 RAG 구조 이해를 위한 뼈대코드(skeleton code) 입니다.\n",
    "\n",
    "각 단계별 모듈의 내용을 앞으로 상황에 맞게 변경하면서 문서에 적합한 구조를 찾아갈 수 있습니다.\n",
    "\n",
    "(각 단계별로 다양한 옵션을 설정하거나 새로운 기법을 적용할 수 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "id": "377894c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:55:19.713897Z",
     "start_time": "2025-02-26T08:55:18.538896Z"
    }
   },
   "source": [
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PyMuPDFLoader(\"data/01. 온보딩 프로세스 - 기반기술.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 페이지수: {len(docs)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 페이지수: 11\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "3b34f4fe",
   "metadata": {},
   "source": [
    "페이지의 내용을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "ddf0d7c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:55:30.550120Z",
     "start_time": "2025-02-26T08:55:30.546745Z"
    }
   },
   "source": "print(docs[10].page_content)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4. Hazelcast (Cache, IMDG- In-Memory Data Grid) \n",
      "분산형 메모리 스토어에요 \n",
      "DWorks 는 Spring Cache 저장소로 사용하고 있어요 \n",
      "Spring Cache 는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, \n",
      "@CachePut 과 같은 Annotation 을 사용하여 캐싱기능을 쉽게 구현 가능하게해줘요 \n",
      "영구적인 데이터 저장소는 DB/Elasticsearch 를 이용하고, 성능향상을 위한 캐싱처리는 \n",
      "Hazelcast 를 이용해요. \n",
      " \n",
      "C팀에서 준비한 자료도 함께 보아주세요! \n",
      " \n",
      "Part_02_기술요소.docx\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "a7e2963b",
   "metadata": {},
   "source": [
    "`metadata` 를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "6d6b05fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:55:35.302654Z",
     "start_time": "2025-02-26T08:55:35.294182Z"
    }
   },
   "source": [
    "docs[10].__dict__"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'producer': 'Skia/PDF m134 Google Docs Renderer',\n",
       "  'creator': '',\n",
       "  'creationdate': '',\n",
       "  'source': 'data/01. 온보딩 프로세스 - 기반기술.pdf',\n",
       "  'file_path': 'data/01. 온보딩 프로세스 - 기반기술.pdf',\n",
       "  'total_pages': 11,\n",
       "  'format': 'PDF 1.4',\n",
       "  'title': '01. 온보딩 프로세스 - 기반기술',\n",
       "  'author': '',\n",
       "  'subject': '',\n",
       "  'keywords': '',\n",
       "  'moddate': '',\n",
       "  'trapped': '',\n",
       "  'modDate': '',\n",
       "  'creationDate': '',\n",
       "  'page': 10},\n",
       " 'page_content': '3.4. Hazelcast (Cache, IMDG- In-Memory Data Grid) \\n분산형 메모리 스토어에요 \\nDWorks 는 Spring Cache 저장소로 사용하고 있어요 \\nSpring Cache 는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, \\n@CachePut 과 같은 Annotation 을 사용하여 캐싱기능을 쉽게 구현 가능하게해줘요 \\n영구적인 데이터 저장소는 DB/Elasticsearch 를 이용하고, 성능향상을 위한 캐싱처리는 \\nHazelcast 를 이용해요. \\n \\nC팀에서 준비한 자료도 함께 보아주세요! \\n \\nPart_02_기술요소.docx',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "1b52f26a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:56:28.760041Z",
     "start_time": "2025-02-26T08:56:28.750267Z"
    }
   },
   "source": [
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"분할된 청크의수: {len(split_documents)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 청크의수: 18\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "795cfec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:57:21.956384Z",
     "start_time": "2025-02-26T08:57:21.803167Z"
    }
   },
   "source": [
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "embeddings = OpenAIEmbeddings()"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "82f47754",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:59:14.113192Z",
     "start_time": "2025-02-26T08:59:12.545944Z"
    }
   },
   "source": [
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = Chroma.from_documents(documents=split_documents, embedding=embeddings)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "34dd3019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:00:29.869117Z",
     "start_time": "2025-02-26T09:00:29.164338Z"
    }
   },
   "source": [
    "for doc in vectorstore.similarity_search(\"분산형 메모리 스토어\"):\n",
    "    print(\"=\" * 50)\n",
    "    print(doc.page_content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(Document(metadata={'author': '', 'creationDate': '', 'creationdate': '', 'creator': '', 'file_path': 'data/01. 온보딩 프로세스 - 기반기술.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 10, 'producer': 'Skia/PDF m134 Google Docs Renderer', 'source': 'data/01. 온보딩 프로세스 - 기반기술.pdf', 'subject': '', 'title': '01. 온보딩 프로세스 - 기반기술', 'total_pages': 11, 'trapped': ''}, page_content='3.4. Hazelcast (Cache, IMDG- In-Memory Data Grid) \\n분산형 메모리 스토어에요 \\nDWorks 는 Spring Cache 저장소로 사용하고 있어요 \\nSpring Cache 는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, \\n@CachePut 과 같은 Annotation 을 사용하여 캐싱기능을 쉽게 구현 가능하게해줘요 \\n영구적인 데이터 저장소는 DB/Elasticsearch 를 이용하고, 성능향상을 위한 캐싱처리는 \\nHazelcast 를 이용해요. \\n \\nC팀에서 준비한 자료도 함께 보아주세요! \\n \\nPart_02_기술요소.docx'), 0.3318576393976306)\n",
      "==================================================\n",
      "(Document(metadata={'author': '', 'creationDate': '', 'creationdate': '', 'creator': '', 'file_path': 'data/01. 온보딩 프로세스 - 기반기술.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 9, 'producer': 'Skia/PDF m134 Google Docs Renderer', 'source': 'data/01. 온보딩 프로세스 - 기반기술.pdf', 'subject': '', 'title': '01. 온보딩 프로세스 - 기반기술', 'total_pages': 11, 'trapped': ''}, page_content='user 같은 것들이죠 \\nb.\\u200b 하나 이상의 shard로 구성이 되요 \\n2.\\u200b shard \\na.\\u200b 데이터를 분산하여 저장하고, 검색작업을 병렬로 처리하기위한 기본 \\n단위에요, 클러스터내의 물리적인 노드들에 분산되어 저장된답니다 \\nb.\\u200b Primary shard, Replica Shard 로 구성되며, 예를들어 Primary shard 가 3, \\nReplica shard 가 1로 설정된다면 데이터가 3개로 나뉘어져 저장되고, 각각 \\n1개씩의 Replica shard 로 구성되요'), 0.3641729140376791)\n",
      "==================================================\n",
      "(Document(metadata={'author': '', 'creationDate': '', 'creationdate': '', 'creator': '', 'file_path': 'data/01. 온보딩 프로세스 - 기반기술.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 8, 'producer': 'Skia/PDF m134 Google Docs Renderer', 'source': 'data/01. 온보딩 프로세스 - 기반기술.pdf', 'subject': '', 'title': '01. 온보딩 프로세스 - 기반기술', 'total_pages': 11, 'trapped': ''}, page_content='3. 인프라 \\n3.1. Kafka (메시지 브로커) \\n분산처리 환경에서 대용량의 메시지(데이터)를 안전하게 처리하기 위한 분산 스트리밍 \\n도구에요 \\n분산 아키텍처, 확장성, 신뢰성있는 데이터 전달등의 특징을 기반으로, 대규모 로그 수집, \\n이벤트 소싱등에서 사용된답니다. \\nKafka 를 사용함으로써 서비스 간의 결합도를 낮추고, 서비스의 독립성과 확장성을 \\n향상시킬 수 있어요! \\n구성 \\n1.\\u200b Broker \\na.\\u200b 클러스터를 구성하는 하나의 노드(서버)를 Broker라고 해요 \\n2.\\u200b Producer \\na.\\u200b 메시지를 생성하는 주체이며, 메시지는 특정 Topic 으로 보내져요 \\nb.\\u200b Kafka 클러스터중 하나의 Broker 에게 보내져요 \\nc.\\u200b Topic 은 메시지의 주소라고 생각하면 됩니다. \\n3.\\u200b Consumer \\na.\\u200b 메시지를 소비하는 주체이고, 특정 Topic 의 메시지를 가져와서 처리합니다.'), 0.36545746000568785)\n",
      "==================================================\n",
      "(Document(metadata={'author': '', 'creationDate': '', 'creationdate': '', 'creator': '', 'file_path': 'data/01. 온보딩 프로세스 - 기반기술.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 0, 'producer': 'Skia/PDF m134 Google Docs Renderer', 'source': 'data/01. 온보딩 프로세스 - 기반기술.pdf', 'subject': '', 'title': '01. 온보딩 프로세스 - 기반기술', 'total_pages': 11, 'trapped': ''}, page_content='1. 온보딩 프로세스 \\n기반 기술 \\n1.1. info \\nOwner \\n \\n서정현\\n \\n1.2. History \\n버전 \\n변경 내용 \\n작성자 \\n일자 \\nv0.1 \\n초안 작성 \\n \\n서정현\\n \\n2024년 1월 15일\\nv1.0 \\n리뷰 의견 적용 및 내용 추가 \\n \\n서정현\\n \\n2024년 1월 17일\\n1.3. 문서의 목적 \\nDWorks 를 처음 접하는 구성원이 DWorks를 이해하기 위한 기반 지식 습득 및 구성원간의 \\n원활한 소통이 가능하도록 합니다. \\n2. 기반 지식 \\n2.1. MSA (MicroServices Architecture) \\nMSA 하면 항상 함께 얘기되는게 Monolithic 아키텍처에요.'), 0.37581183699791076)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "838f7729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:01:14.073263Z",
     "start_time": "2025-02-26T09:01:14.070124Z"
    }
   },
   "source": [
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "f29da7b4",
   "metadata": {},
   "source": [
    "검색기에 쿼리를 날려 검색된 chunk 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "16c0ad82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:02:45.630812Z",
     "start_time": "2025-02-26T09:02:44.865904Z"
    }
   },
   "source": [
    "# 검색기에 쿼리를 날려 검색된 chunk 결과를 확인합니다.\n",
    "docs = retriever.invoke(\"분산형 메모리 스토어가 뭐야?\")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "6e9b613061439e43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:01:29.524783Z",
     "start_time": "2025-02-26T09:01:29.518154Z"
    }
   },
   "source": [
    "for doc in docs:\n",
    "    print(\"=\" * 50)\n",
    "    print(doc.page_content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "3.4. Hazelcast (Cache, IMDG- In-Memory Data Grid) \n",
      "분산형 메모리 스토어에요 \n",
      "DWorks 는 Spring Cache 저장소로 사용하고 있어요 \n",
      "Spring Cache 는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, \n",
      "@CachePut 과 같은 Annotation 을 사용하여 캐싱기능을 쉽게 구현 가능하게해줘요 \n",
      "영구적인 데이터 저장소는 DB/Elasticsearch 를 이용하고, 성능향상을 위한 캐싱처리는 \n",
      "Hazelcast 를 이용해요. \n",
      " \n",
      "C팀에서 준비한 자료도 함께 보아주세요! \n",
      " \n",
      "Part_02_기술요소.docx\n",
      "==================================================\n",
      "user 같은 것들이죠 \n",
      "b.​ 하나 이상의 shard로 구성이 되요 \n",
      "2.​ shard \n",
      "a.​ 데이터를 분산하여 저장하고, 검색작업을 병렬로 처리하기위한 기본 \n",
      "단위에요, 클러스터내의 물리적인 노드들에 분산되어 저장된답니다 \n",
      "b.​ Primary shard, Replica Shard 로 구성되며, 예를들어 Primary shard 가 3, \n",
      "Replica shard 가 1로 설정된다면 데이터가 3개로 나뉘어져 저장되고, 각각 \n",
      "1개씩의 Replica shard 로 구성되요\n",
      "==================================================\n",
      "3. 인프라 \n",
      "3.1. Kafka (메시지 브로커) \n",
      "분산처리 환경에서 대용량의 메시지(데이터)를 안전하게 처리하기 위한 분산 스트리밍 \n",
      "도구에요 \n",
      "분산 아키텍처, 확장성, 신뢰성있는 데이터 전달등의 특징을 기반으로, 대규모 로그 수집, \n",
      "이벤트 소싱등에서 사용된답니다. \n",
      "Kafka 를 사용함으로써 서비스 간의 결합도를 낮추고, 서비스의 독립성과 확장성을 \n",
      "향상시킬 수 있어요! \n",
      "구성 \n",
      "1.​ Broker \n",
      "a.​ 클러스터를 구성하는 하나의 노드(서버)를 Broker라고 해요 \n",
      "2.​ Producer \n",
      "a.​ 메시지를 생성하는 주체이며, 메시지는 특정 Topic 으로 보내져요 \n",
      "b.​ Kafka 클러스터중 하나의 Broker 에게 보내져요 \n",
      "c.​ Topic 은 메시지의 주소라고 생각하면 됩니다. \n",
      "3.​ Consumer \n",
      "a.​ 메시지를 소비하는 주체이고, 특정 Topic 의 메시지를 가져와서 처리합니다.\n",
      "==================================================\n",
      "b.​ Consumer Group 에 속할 수 있고, 여러 Consumer 가 동시에 처리 할 수도 \n",
      "있어요 \n",
      "3.2. ZooKeeper (Coordination System) \n",
      "Kafka 클러스터 구성 정보, Broker의 상태(Leader/Follower 선출) 및 Topic 의 구성등을 \n",
      "저장하고 관리해요 \n",
      " \n",
      "3.3. Elasticsearch (검색엔진) \n",
      "Apache Lucene 기반의 분산 검색 엔진이에요 \n",
      "아래와 같은 특징을 가지고 있어요. \n",
      "1.​ 분산환경에서 여러 노드에 대량의 데이터를 분산저장 및 처리 할수 있어요 \n",
      "2.​ 실시간으로 데이터를 색인하고 검색하는데 강점을 가지고 있어요 \n",
      "3.​ RESTful API 를 통해 데이터를 저장/색인/분석/조회 할수있어요 \n",
      "구성 \n",
      "1.​ index \n",
      "a.​ 데이터를 저장하고 검색하기위한 논리적인 공간이에요, message, ticket, \n",
      "user 같은 것들이죠 \n",
      "b.​ 하나 이상의 shard로 구성이 되요\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "3bb3e26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:02:09.427290Z",
     "start_time": "2025-02-26T09:02:09.423206Z"
    }
   },
   "source": [
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "669ed5b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:02:10.750745Z",
     "start_time": "2025-02-26T09:02:10.691777Z"
    }
   },
   "source": [
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "3113bc05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:05:13.749339Z",
     "start_time": "2025-02-26T09:05:13.745794Z"
    }
   },
   "source": [
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "e79f4aeb",
   "metadata": {},
   "source": [
    "생성된 체인에 쿼리(질문)을 입력하고 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "50d6b7f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:05:20.294251Z",
     "start_time": "2025-02-26T09:05:16.863469Z"
    }
   },
   "source": [
    "# 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"분산형 메모리 스토어에 대해 설명해줘\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"분산형 메모리 스토어에 대해 설명해줘\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"분산형 메모리 스토어에 대해 설명해줘\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"분산형 메모리 스토어에 대해 설명해줘\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"분산형 메모리 스토어에 대해 설명해줘\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"분산형 메모리 스토어에 대해 설명해줘\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:format_docs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:format_docs] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"3.4. Hazelcast (Cache, IMDG- In-Memory Data Grid) \\n분산형 메모리 스토어에요 \\nDWorks 는 Spring Cache 저장소로 사용하고 있어요 \\nSpring Cache 는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, \\n@CachePut 과 같은 Annotation 을 사용하여 캐싱기능을 쉽게 구현 가능하게해줘요 \\n영구적인 데이터 저장소는 DB/Elasticsearch 를 이용하고, 성능향상을 위한 캐싱처리는 \\nHazelcast 를 이용해요. \\n \\nC팀에서 준비한 자료도 함께 보아주세요! \\n \\nPart_02_기술요소.docx\\n\\nuser 같은 것들이죠 \\nb.​ 하나 이상의 shard로 구성이 되요 \\n2.​ shard \\na.​ 데이터를 분산하여 저장하고, 검색작업을 병렬로 처리하기위한 기본 \\n단위에요, 클러스터내의 물리적인 노드들에 분산되어 저장된답니다 \\nb.​ Primary shard, Replica Shard 로 구성되며, 예를들어 Primary shard 가 3, \\nReplica shard 가 1로 설정된다면 데이터가 3개로 나뉘어져 저장되고, 각각 \\n1개씩의 Replica shard 로 구성되요\\n\\n1. 온보딩 프로세스 \\n기반 기술 \\n1.1. info \\nOwner \\n \\n서정현\\n \\n1.2. History \\n버전 \\n변경 내용 \\n작성자 \\n일자 \\nv0.1 \\n초안 작성 \\n \\n서정현\\n \\n2024년 1월 15일\\nv1.0 \\n리뷰 의견 적용 및 내용 추가 \\n \\n서정현\\n \\n2024년 1월 17일\\n1.3. 문서의 목적 \\nDWorks 를 처음 접하는 구성원이 DWorks를 이해하기 위한 기반 지식 습득 및 구성원간의 \\n원활한 소통이 가능하도록 합니다. \\n2. 기반 지식 \\n2.1. MSA (MicroServices Architecture) \\nMSA 하면 항상 함께 얘기되는게 Monolithic 아키텍처에요.\\n\\n3. 인프라 \\n3.1. Kafka (메시지 브로커) \\n분산처리 환경에서 대용량의 메시지(데이터)를 안전하게 처리하기 위한 분산 스트리밍 \\n도구에요 \\n분산 아키텍처, 확장성, 신뢰성있는 데이터 전달등의 특징을 기반으로, 대규모 로그 수집, \\n이벤트 소싱등에서 사용된답니다. \\nKafka 를 사용함으로써 서비스 간의 결합도를 낮추고, 서비스의 독립성과 확장성을 \\n향상시킬 수 있어요! \\n구성 \\n1.​ Broker \\na.​ 클러스터를 구성하는 하나의 노드(서버)를 Broker라고 해요 \\n2.​ Producer \\na.​ 메시지를 생성하는 주체이며, 메시지는 특정 Topic 으로 보내져요 \\nb.​ Kafka 클러스터중 하나의 Broker 에게 보내져요 \\nc.​ Topic 은 메시지의 주소라고 생각하면 됩니다. \\n3.​ Consumer \\na.​ 메시지를 소비하는 주체이고, 특정 Topic 의 메시지를 가져와서 처리합니다.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] [540ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"3.4. Hazelcast (Cache, IMDG- In-Memory Data Grid) \\n분산형 메모리 스토어에요 \\nDWorks 는 Spring Cache 저장소로 사용하고 있어요 \\nSpring Cache 는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, \\n@CachePut 과 같은 Annotation 을 사용하여 캐싱기능을 쉽게 구현 가능하게해줘요 \\n영구적인 데이터 저장소는 DB/Elasticsearch 를 이용하고, 성능향상을 위한 캐싱처리는 \\nHazelcast 를 이용해요. \\n \\nC팀에서 준비한 자료도 함께 보아주세요! \\n \\nPart_02_기술요소.docx\\n\\nuser 같은 것들이죠 \\nb.​ 하나 이상의 shard로 구성이 되요 \\n2.​ shard \\na.​ 데이터를 분산하여 저장하고, 검색작업을 병렬로 처리하기위한 기본 \\n단위에요, 클러스터내의 물리적인 노드들에 분산되어 저장된답니다 \\nb.​ Primary shard, Replica Shard 로 구성되며, 예를들어 Primary shard 가 3, \\nReplica shard 가 1로 설정된다면 데이터가 3개로 나뉘어져 저장되고, 각각 \\n1개씩의 Replica shard 로 구성되요\\n\\n1. 온보딩 프로세스 \\n기반 기술 \\n1.1. info \\nOwner \\n \\n서정현\\n \\n1.2. History \\n버전 \\n변경 내용 \\n작성자 \\n일자 \\nv0.1 \\n초안 작성 \\n \\n서정현\\n \\n2024년 1월 15일\\nv1.0 \\n리뷰 의견 적용 및 내용 추가 \\n \\n서정현\\n \\n2024년 1월 17일\\n1.3. 문서의 목적 \\nDWorks 를 처음 접하는 구성원이 DWorks를 이해하기 위한 기반 지식 습득 및 구성원간의 \\n원활한 소통이 가능하도록 합니다. \\n2. 기반 지식 \\n2.1. MSA (MicroServices Architecture) \\nMSA 하면 항상 함께 얘기되는게 Monolithic 아키텍처에요.\\n\\n3. 인프라 \\n3.1. Kafka (메시지 브로커) \\n분산처리 환경에서 대용량의 메시지(데이터)를 안전하게 처리하기 위한 분산 스트리밍 \\n도구에요 \\n분산 아키텍처, 확장성, 신뢰성있는 데이터 전달등의 특징을 기반으로, 대규모 로그 수집, \\n이벤트 소싱등에서 사용된답니다. \\nKafka 를 사용함으로써 서비스 간의 결합도를 낮추고, 서비스의 독립성과 확장성을 \\n향상시킬 수 있어요! \\n구성 \\n1.​ Broker \\na.​ 클러스터를 구성하는 하나의 노드(서버)를 Broker라고 해요 \\n2.​ Producer \\na.​ 메시지를 생성하는 주체이며, 메시지는 특정 Topic 으로 보내져요 \\nb.​ Kafka 클러스터중 하나의 Broker 에게 보내져요 \\nc.​ Topic 은 메시지의 주소라고 생각하면 됩니다. \\n3.​ Consumer \\na.​ 메시지를 소비하는 주체이고, 특정 Topic 의 메시지를 가져와서 처리합니다.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [541ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"context\": \"3.4. Hazelcast (Cache, IMDG- In-Memory Data Grid) \\n분산형 메모리 스토어에요 \\nDWorks 는 Spring Cache 저장소로 사용하고 있어요 \\nSpring Cache 는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, \\n@CachePut 과 같은 Annotation 을 사용하여 캐싱기능을 쉽게 구현 가능하게해줘요 \\n영구적인 데이터 저장소는 DB/Elasticsearch 를 이용하고, 성능향상을 위한 캐싱처리는 \\nHazelcast 를 이용해요. \\n \\nC팀에서 준비한 자료도 함께 보아주세요! \\n \\nPart_02_기술요소.docx\\n\\nuser 같은 것들이죠 \\nb.​ 하나 이상의 shard로 구성이 되요 \\n2.​ shard \\na.​ 데이터를 분산하여 저장하고, 검색작업을 병렬로 처리하기위한 기본 \\n단위에요, 클러스터내의 물리적인 노드들에 분산되어 저장된답니다 \\nb.​ Primary shard, Replica Shard 로 구성되며, 예를들어 Primary shard 가 3, \\nReplica shard 가 1로 설정된다면 데이터가 3개로 나뉘어져 저장되고, 각각 \\n1개씩의 Replica shard 로 구성되요\\n\\n1. 온보딩 프로세스 \\n기반 기술 \\n1.1. info \\nOwner \\n \\n서정현\\n \\n1.2. History \\n버전 \\n변경 내용 \\n작성자 \\n일자 \\nv0.1 \\n초안 작성 \\n \\n서정현\\n \\n2024년 1월 15일\\nv1.0 \\n리뷰 의견 적용 및 내용 추가 \\n \\n서정현\\n \\n2024년 1월 17일\\n1.3. 문서의 목적 \\nDWorks 를 처음 접하는 구성원이 DWorks를 이해하기 위한 기반 지식 습득 및 구성원간의 \\n원활한 소통이 가능하도록 합니다. \\n2. 기반 지식 \\n2.1. MSA (MicroServices Architecture) \\nMSA 하면 항상 함께 얘기되는게 Monolithic 아키텍처에요.\\n\\n3. 인프라 \\n3.1. Kafka (메시지 브로커) \\n분산처리 환경에서 대용량의 메시지(데이터)를 안전하게 처리하기 위한 분산 스트리밍 \\n도구에요 \\n분산 아키텍처, 확장성, 신뢰성있는 데이터 전달등의 특징을 기반으로, 대규모 로그 수집, \\n이벤트 소싱등에서 사용된답니다. \\nKafka 를 사용함으로써 서비스 간의 결합도를 낮추고, 서비스의 독립성과 확장성을 \\n향상시킬 수 있어요! \\n구성 \\n1.​ Broker \\na.​ 클러스터를 구성하는 하나의 노드(서버)를 Broker라고 해요 \\n2.​ Producer \\na.​ 메시지를 생성하는 주체이며, 메시지는 특정 Topic 으로 보내져요 \\nb.​ Kafka 클러스터중 하나의 Broker 에게 보내져요 \\nc.​ Topic 은 메시지의 주소라고 생각하면 됩니다. \\n3.​ Consumer \\na.​ 메시지를 소비하는 주체이고, 특정 Topic 의 메시지를 가져와서 처리합니다.\",\n",
      "  \"question\": \"분산형 메모리 스토어에 대해 설명해줘\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m{\n",
      "  \"context\": \"3.4. Hazelcast (Cache, IMDG- In-Memory Data Grid) \\n분산형 메모리 스토어에요 \\nDWorks 는 Spring Cache 저장소로 사용하고 있어요 \\nSpring Cache 는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, \\n@CachePut 과 같은 Annotation 을 사용하여 캐싱기능을 쉽게 구현 가능하게해줘요 \\n영구적인 데이터 저장소는 DB/Elasticsearch 를 이용하고, 성능향상을 위한 캐싱처리는 \\nHazelcast 를 이용해요. \\n \\nC팀에서 준비한 자료도 함께 보아주세요! \\n \\nPart_02_기술요소.docx\\n\\nuser 같은 것들이죠 \\nb.​ 하나 이상의 shard로 구성이 되요 \\n2.​ shard \\na.​ 데이터를 분산하여 저장하고, 검색작업을 병렬로 처리하기위한 기본 \\n단위에요, 클러스터내의 물리적인 노드들에 분산되어 저장된답니다 \\nb.​ Primary shard, Replica Shard 로 구성되며, 예를들어 Primary shard 가 3, \\nReplica shard 가 1로 설정된다면 데이터가 3개로 나뉘어져 저장되고, 각각 \\n1개씩의 Replica shard 로 구성되요\\n\\n1. 온보딩 프로세스 \\n기반 기술 \\n1.1. info \\nOwner \\n \\n서정현\\n \\n1.2. History \\n버전 \\n변경 내용 \\n작성자 \\n일자 \\nv0.1 \\n초안 작성 \\n \\n서정현\\n \\n2024년 1월 15일\\nv1.0 \\n리뷰 의견 적용 및 내용 추가 \\n \\n서정현\\n \\n2024년 1월 17일\\n1.3. 문서의 목적 \\nDWorks 를 처음 접하는 구성원이 DWorks를 이해하기 위한 기반 지식 습득 및 구성원간의 \\n원활한 소통이 가능하도록 합니다. \\n2. 기반 지식 \\n2.1. MSA (MicroServices Architecture) \\nMSA 하면 항상 함께 얘기되는게 Monolithic 아키텍처에요.\\n\\n3. 인프라 \\n3.1. Kafka (메시지 브로커) \\n분산처리 환경에서 대용량의 메시지(데이터)를 안전하게 처리하기 위한 분산 스트리밍 \\n도구에요 \\n분산 아키텍처, 확장성, 신뢰성있는 데이터 전달등의 특징을 기반으로, 대규모 로그 수집, \\n이벤트 소싱등에서 사용된답니다. \\nKafka 를 사용함으로써 서비스 간의 결합도를 낮추고, 서비스의 독립성과 확장성을 \\n향상시킬 수 있어요! \\n구성 \\n1.​ Broker \\na.​ 클러스터를 구성하는 하나의 노드(서버)를 Broker라고 해요 \\n2.​ Producer \\na.​ 메시지를 생성하는 주체이며, 메시지는 특정 Topic 으로 보내져요 \\nb.​ Kafka 클러스터중 하나의 Broker 에게 보내져요 \\nc.​ Topic 은 메시지의 주소라고 생각하면 됩니다. \\n3.​ Consumer \\na.​ 메시지를 소비하는 주체이고, 특정 Topic 의 메시지를 가져와서 처리합니다.\",\n",
      "  \"question\": \"분산형 메모리 스토어에 대해 설명해줘\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nAnswer in Korean.\\n\\n#Context: \\n3.4. Hazelcast (Cache, IMDG- In-Memory Data Grid) \\n분산형 메모리 스토어에요 \\nDWorks 는 Spring Cache 저장소로 사용하고 있어요 \\nSpring Cache 는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, \\n@CachePut 과 같은 Annotation 을 사용하여 캐싱기능을 쉽게 구현 가능하게해줘요 \\n영구적인 데이터 저장소는 DB/Elasticsearch 를 이용하고, 성능향상을 위한 캐싱처리는 \\nHazelcast 를 이용해요. \\n \\nC팀에서 준비한 자료도 함께 보아주세요! \\n \\nPart_02_기술요소.docx\\n\\nuser 같은 것들이죠 \\nb.​ 하나 이상의 shard로 구성이 되요 \\n2.​ shard \\na.​ 데이터를 분산하여 저장하고, 검색작업을 병렬로 처리하기위한 기본 \\n단위에요, 클러스터내의 물리적인 노드들에 분산되어 저장된답니다 \\nb.​ Primary shard, Replica Shard 로 구성되며, 예를들어 Primary shard 가 3, \\nReplica shard 가 1로 설정된다면 데이터가 3개로 나뉘어져 저장되고, 각각 \\n1개씩의 Replica shard 로 구성되요\\n\\n1. 온보딩 프로세스 \\n기반 기술 \\n1.1. info \\nOwner \\n \\n서정현\\n \\n1.2. History \\n버전 \\n변경 내용 \\n작성자 \\n일자 \\nv0.1 \\n초안 작성 \\n \\n서정현\\n \\n2024년 1월 15일\\nv1.0 \\n리뷰 의견 적용 및 내용 추가 \\n \\n서정현\\n \\n2024년 1월 17일\\n1.3. 문서의 목적 \\nDWorks 를 처음 접하는 구성원이 DWorks를 이해하기 위한 기반 지식 습득 및 구성원간의 \\n원활한 소통이 가능하도록 합니다. \\n2. 기반 지식 \\n2.1. MSA (MicroServices Architecture) \\nMSA 하면 항상 함께 얘기되는게 Monolithic 아키텍처에요.\\n\\n3. 인프라 \\n3.1. Kafka (메시지 브로커) \\n분산처리 환경에서 대용량의 메시지(데이터)를 안전하게 처리하기 위한 분산 스트리밍 \\n도구에요 \\n분산 아키텍처, 확장성, 신뢰성있는 데이터 전달등의 특징을 기반으로, 대규모 로그 수집, \\n이벤트 소싱등에서 사용된답니다. \\nKafka 를 사용함으로써 서비스 간의 결합도를 낮추고, 서비스의 독립성과 확장성을 \\n향상시킬 수 있어요! \\n구성 \\n1.​ Broker \\na.​ 클러스터를 구성하는 하나의 노드(서버)를 Broker라고 해요 \\n2.​ Producer \\na.​ 메시지를 생성하는 주체이며, 메시지는 특정 Topic 으로 보내져요 \\nb.​ Kafka 클러스터중 하나의 Broker 에게 보내져요 \\nc.​ Topic 은 메시지의 주소라고 생각하면 됩니다. \\n3.​ Consumer \\na.​ 메시지를 소비하는 주체이고, 특정 Topic 의 메시지를 가져와서 처리합니다.\\n\\n#Question:\\n분산형 메모리 스토어에 대해 설명해줘\\n\\n#Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.88s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"분산형 메모리 스토어는 데이터를 여러 노드에 분산하여 저장하고, 이를 통해 빠른 데이터 접근과 처리를 가능하게 하는 시스템입니다. 예를 들어, Hazelcast는 이러한 분산형 메모리 스토어의 한 예로, DWorks에서 Spring Cache 저장소로 사용되고 있습니다. Spring Cache는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, @CachePut과 같은 애너테이션을 사용하여 캐싱 기능을 쉽게 구현할 수 있도록 도와줍니다. 이러한 시스템은 영구적인 데이터 저장소와 함께 사용되어 성능을 향상시키는 데 기여합니다.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"분산형 메모리 스토어는 데이터를 여러 노드에 분산하여 저장하고, 이를 통해 빠른 데이터 접근과 처리를 가능하게 하는 시스템입니다. 예를 들어, Hazelcast는 이러한 분산형 메모리 스토어의 한 예로, DWorks에서 Spring Cache 저장소로 사용되고 있습니다. Spring Cache는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, @CachePut과 같은 애너테이션을 사용하여 캐싱 기능을 쉽게 구현할 수 있도록 도와줍니다. 이러한 시스템은 영구적인 데이터 저장소와 함께 사용되어 성능을 향상시키는 데 기여합니다.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 147,\n",
      "                \"prompt_tokens\": 768,\n",
      "                \"total_tokens\": 915,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_06737a9306\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5ef91b50-a069-47e0-8366-6477454351b1-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 768,\n",
      "              \"output_tokens\": 147,\n",
      "              \"total_tokens\": 915,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 147,\n",
      "      \"prompt_tokens\": 768,\n",
      "      \"total_tokens\": 915,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_06737a9306\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"분산형 메모리 스토어는 데이터를 여러 노드에 분산하여 저장하고, 이를 통해 빠른 데이터 접근과 처리를 가능하게 하는 시스템입니다. 예를 들어, Hazelcast는 이러한 분산형 메모리 스토어의 한 예로, DWorks에서 Spring Cache 저장소로 사용되고 있습니다. Spring Cache는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, @CachePut과 같은 애너테이션을 사용하여 캐싱 기능을 쉽게 구현할 수 있도록 도와줍니다. 이러한 시스템은 영구적인 데이터 저장소와 함께 사용되어 성능을 향상시키는 데 기여합니다.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [3.43s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"분산형 메모리 스토어는 데이터를 여러 노드에 분산하여 저장하고, 이를 통해 빠른 데이터 접근과 처리를 가능하게 하는 시스템입니다. 예를 들어, Hazelcast는 이러한 분산형 메모리 스토어의 한 예로, DWorks에서 Spring Cache 저장소로 사용되고 있습니다. Spring Cache는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, @CachePut과 같은 애너테이션을 사용하여 캐싱 기능을 쉽게 구현할 수 있도록 도와줍니다. 이러한 시스템은 영구적인 데이터 저장소와 함께 사용되어 성능을 향상시키는 데 기여합니다.\"\n",
      "}\n",
      "분산형 메모리 스토어는 데이터를 여러 노드에 분산하여 저장하고, 이를 통해 빠른 데이터 접근과 처리를 가능하게 하는 시스템입니다. 예를 들어, Hazelcast는 이러한 분산형 메모리 스토어의 한 예로, DWorks에서 Spring Cache 저장소로 사용되고 있습니다. Spring Cache는 분산 캐싱을 추상화하여 제공하며, @Cacheable, @CacheEvict, @CachePut과 같은 애너테이션을 사용하여 캐싱 기능을 쉽게 구현할 수 있도록 도와줍니다. 이러한 시스템은 영구적인 데이터 저장소와 함께 사용되어 성능을 향상시키는 데 기여합니다.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "f8444e43",
   "metadata": {},
   "source": [
    "## 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "id": "adcfef0209b3ca2",
   "metadata": {},
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PyMuPDFLoader(\"data/01. 온보딩 프로세스 - 기반기술.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = Chroma.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "adc45dbf",
   "metadata": {},
   "source": [
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5986cab",
   "metadata": {},
   "source": [
    "# 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"분산형 메모리 스토어가 뭐야?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
