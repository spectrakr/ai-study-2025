{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory\n",
    "\n",
    "이 메모리는 메시지를 저장한 다음 변수에 메시지를 추출할 수 있게 해줍니다.\n",
    "\n",
    "먼저 문자열로 추출할 수 있습니다.\n",
    "\n",
    "> 이 기능은 1.0에서 deprecated될 예정이다.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 기본적으로 이전 메시지를 모르는 LLM"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "llm.invoke(\"1+1은 뭐야?\").content",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "llm.invoke(\"그럼 거기에 3을 더하면\").content",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ConversationBufferMemory"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "memory.save_context(\n",
    "    inputs={\n",
    "        \"human\": \"1+1은?\"\n",
    "    },\n",
    "    outputs={\n",
    "        \"ai\": \"2입니다\"\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "memory"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memory 의 `load_memory_variables({})` 함수는 메시지 히스토리를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 'history' 키에 저장된 대화 기록을 확인합니다.\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`save_context(inputs, outputs)` 메서드를 사용하여 대화 기록을 저장할 수 있습니다.\n",
    "\n",
    "- 이 메서드는 `inputs`와 `outputs` 두 개의 인자를 받습니다.\n",
    "- `inputs`는 사용자의 입력을, `outputs`는 AI의 출력을 저장합니다.\n",
    "- 이 메서드를 사용하면 대화 기록이 `history` 키에 저장됩니다.\n",
    "- 이후 `load_memory_variables` 메서드를 사용하여 저장된 대화 기록을 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# inputs: dictionary(key: \"human\" or \"ai\", value: 질문)\n",
    "# outputs: dictionary(key: \"ai\" or \"human\", value: 답변)\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"2+2는?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"4입니다\"\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2개의 대화를 저장합니다.\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"3+3은?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"6입니다\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"4+4는?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"8입니다\"\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# history에 저장된 대화 기록을 확인합니다.\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 추가로 2개의 대화를 저장합니다.\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"5+5는?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"10입니다\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"마지막으로 10+10은?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"20입니다\"\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# history에 저장된 대화 기록을 확인합니다.\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`return_messages=True` 로 설정하면 `HumanMessage` 와 `AIMessage` 객체를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\n",
    "        \"human\": \"1+1은?\"\n",
    "    },\n",
    "    outputs={\n",
    "        \"ai\": \"2입니다\"\n",
    "    },\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\n",
    "        \"human\": \"2+2는?\"\n",
    "    },\n",
    "    outputs={\n",
    "        \"ai\": \"4입니다\"\n",
    "    },\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\n",
    "        \"human\": \"3+3은?\"\n",
    "    },\n",
    "    outputs={\n",
    "        \"ai\": \"6입니다\"\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# history에 저장된 대화 기록을 확인합니다.\n",
    "memory.load_memory_variables({})[\"history\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain 에 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# LLM 모델을 생성합니다.\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# ConversationChain을 생성합니다.\n",
    "conversation = ConversationChain(\n",
    "    # ConversationBufferMemory를 사용합니다.\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConversationChain`을 사용하여 대화를 진행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 대화를 시작합니다.\n",
    "response = conversation.predict(\n",
    "    input=\"2+2는 뭐야?\"\n",
    ")\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 대화 기록을 기억하고 있는지 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 이전 답변결과에 4를 더하면\n",
    "response = conversation.predict(\n",
    "    input=\"거기에 4를 더하면?\"\n",
    ")\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
