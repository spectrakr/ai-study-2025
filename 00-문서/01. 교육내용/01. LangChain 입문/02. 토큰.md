
* 토큰(Token)은 자연어처리 NLP에서 텍스트를 작은 단위로 나누어 처리하기 위해 사용 되는 기본단위이다.
* 단어, 부분 단어, 문자 등이 토큰이 될 수 있다.
* LLM(대형 언어 모델)에서 토큰은 텍스트 데이터를 모델이 이해하고 처리하기 위해 분할된 기본단위이다. 텍스트를 토큰으로 나누는 과정을 토큰화라고 한다.

## 토큰화의 방법  
- 문자 기반 토큰화: 텍스트를 문자 단위로 나눔
	- 예시: "Hello, world" --> ["H", "e", "l", "l", "o", ",", "w", "o", "r", "l", "d"] (11 토큰)
- 단어 기반 토큰화: 텍스트를 단어 단위로 나누는 방법
	- 예시: "Hello, world" --> ["Hello", ",", "world"] (3 토큰)
- 서브워드 기반 토큰화: 단어를 더 작은 단위로 나눔
	- 예시: "unhappiness" --> ["un", "happiness"]
	- 예시: "우리나라 만세" --> ["우리", "나라", "만", "세"]
	- BPE(Byte Pair Encoding): 자주 등장하는 문자 쌍을 합쳐가면서 서브워드를 생성하는 알고리즘

>LLM은 다음에 나올 단어를 확률적으로 예측을 하고 가장 높은 토큰을 선택한다.
  문자 기반은 글자를 너무 잘게 쪼개서 다음에 올바른 단어를 선택하는 확률이 낮아진다.
>단어 기반은 단어 사전을 만들어야 하는데 유사한 의미라도 개별 단어로 인식을 해야 한다. 
>- 단어기반) cloud, cloudy, cloudless
>- 서브워드기반) `cloud`, `cloud`y, `cloud`less
>현재 LLM은 서브워드 기반 토큰화를 사용한다.


## 토큰 사용량 = 돈$
모델별로 토큰을 짜르는 방식이 다를 수 있다. 그래서 같은 문장이라도 토큰 수가 달라진다.

| 모델                           | 입력 토큰 100만 개 | 출력 토큰 100만 개 |
| ---------------------------- | ------------ | ------------ |
| GPT-3.5                      | $0.5         | $1.5         |
| GPT-4 Turbo                  | $10          | $30          |
| GPT-4o                       | $5           | $15          |
| GPT-4o mini                  | $0.15        | $0.60        |
| text-embedding-3-small (임베딩) | $0.02        |              |
토큰계산기: https://tiktokenizer.vercel.app/?model=gpt-4o
- 대한민국의 수도는 서울입니다
- gpt-4o: 14 token
- gpt-3.5-turbo: 22 token
모델별 토큰계산: https://gptforwork.com/tools/openai-chatgpt-api-pricing-calculator


## Context Length
Context Length는 LLM이 한번에 처리할 수 있는 최대 토큰 수를 의미한다.

|    Model    | Context Window 크기 |
| :---------: | :---------------: |
|   GPT 3.5   |        16K        |
| GPT 4-Turbo |       128K        |
|   GPT-4o    |       128K        |
> 입력과 출력을 포함한 크기이다.


## max_tokens 
답변에 대한 최대 출력 토큰 수

|    Model    | max_tokens |
| :---------: | :--------: |
|   GPT 3.5   |    4096    |
| GPT 4-Turbo |    4096    |
|   GPT-4o    |    4096    |
> gpt-4o-2024-0806은 16384 토큰까지 지원