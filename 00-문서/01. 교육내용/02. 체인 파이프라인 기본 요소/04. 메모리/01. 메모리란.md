LLM과의 대화중이 이전 내용을 기반으로 답변을 해주는 기능
- 이전에 내가 했던 말을 요약해줘
GPT는 사용자의 세션 정보를 가지고 있지 않다. 즉, 이전에 무슨 말을 했는지 기억하지 못한다.
기억하려면 매 요청 시 이전 메시지를 같이 전달해야 한다.

memory는 질문과 답변의 쌍으로 저장을 한다.
```python
memory.save_context(  
    inputs={  
        "human": "안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?"  
    },  
    outputs={  
        "ai": "안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?"  
    },  
)
```

```python
ConversationBufferMemory(
	chat_memory=InMemoryChatMessageHistory(
		messages=[
			HumanMessage(content='안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?', additional_kwargs={}, response_metadata={}), 
			AIMessage(content='안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?', additional_kwargs={}, response_metadata={})
		]
	)
)
```


#### ConversationalBufferMemory
- 메시지 내용을 기억

#### ConversationBufferWindowMemory
- llm의 context 크기를 초과하거나 비용 문제가 발생할 수 있다.
- 최근 k개의 메시지를 기억

```python
memory = ConversationBufferWindowMemory(k=2, return_messages=True)
```

#### ConversationTokenBufferMemory
- 토큰 길이로 저장

```python
memory = ConversationTokenBufferMemory(  
    llm=llm, max_token_limit=150, return_messages=True  # 최대 토큰 길이를 50개로 제한  
)
```

#### ConversationEntityMemory
- 대화에서 특정 엔티티에 대한 사실을 기억

#### ConversationKnowledgeGraph
- 지식 그래프의 힘을 활용하여 정보를 저장하고 불러온다

#### ConversationSummaryMemory
- 대화를 요약하고 메모리에 저장한다

#### ConversationSummaryBufferMemory
- 최근 대화내용의 버퍼를 메모리에 유지하고, 이전 대화내용은 요약하여 저장한다.
```python
memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=200,  # 요약의 기준이 되는 토큰 길이를 설정합니다.
    return_messages=True,
)
```

#### VectorStoreRetrieverMemory
- 벡터 스토어에 메모리를 저장하고 호출될 때마다 가장 '눈에 띄는' 상위 K개의 문서를 쿼리한다.


