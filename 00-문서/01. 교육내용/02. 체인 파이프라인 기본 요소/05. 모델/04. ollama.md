
## 1. ollama 설치
https://ollama.com/

설치 후 재부팅이 필요할 수도 있다.

## 2. 모델 설치

#### 방법 1: ollama pull설치
- Ollama 공식 저장소에서 설치
- 장점
	- 명령어로 간단히 설치
	- 간단한 명령어로 업데이트 가능 - ollama pull <모델이름>
- 단점
	- Ollama에서 제공하는 모델만 가능
	- 네트워크 필요
	- 기본적으로 Modelfile 설정 변경 불가 (우회 방법으로는 가능)

#### 방법 2: 모델 파일 직접 다운로드 & 설치
- 모델파일 (gguf) 다운로드 및 설치
- 장점
	- Hugging Face에 있는 모델 설치 가능
	- Modelfile 설정 변경 가능
	- 네트워크 불필요
- 단점
	- 설정 복잡할 수 있음
	- 모델 업데이트 시 다시 다운로드
	




## 3. 모델 파일 다운로드
 - EEVE
	 - 야놀자에서 라마2를 기반으로 한글이 잘되도록 튜닝을 한 모델
	 - 다운로드: https://huggingface.co/teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf/tree/main
- deepseek r1
	- DeepSeek-R1모델을 경량화하고 Qwen 기반으로 14B 파라미터 규모로 구현한 모델
	- 다운로드: https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF/tree/main
	- https://wikidocs.net/274497
- 
- exaone 3.5
	- LG AI Research에서 개발한 AI 모델. 한국어, 영어를 잘하는 모델
	- 다운로드: https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-GGUF/tree/main
	- https://wikidocs.net/274703
-  kanana
	- 카카오에서 만든 AI 모델 (경량)
	- 다운로드: https://huggingface.co/muzerai/kanana-nano-2.1b-instruct-creative-de-aijoa-GGUF/tree/main
	- https://tech.kakao.com/posts/660
- phi-4
	- Microsoft에서 만든 멀티모달 모델
	- 다운로드: https://huggingface.co/microsoft/phi-4-gguf/tree/main
- dna-r1
	- 무엇보다 한국어 기반으로 추론하는 첫 공개 모델이며, DeepSeek-R1과 동일한 방법론(GRPO)으로 대규모 강화학습(RL) 적용, 성능 저하 없이 한국어/영어 이중 언어 지원, 이외에도 수학, 코딩 및 일반 추론 작업에서 뛰어난 능력을 발휘합니다
	- 다운로드: https://huggingface.co/DevQuasar/dnotitia.DNA-R1-GGUF/tree/main
- Gemma3
	- 구글에서 만든 언어모델. 
	- 다운로드: https://huggingface.co/cortexso/gemma3/tree/main
- Qwen
	- 알리바바에서 만든 언어모델. 한국어 이해도가 높다.
	- 다운로드: https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/tree/main


#### GGUF란 무엇인가?

- GGUF와 GGML은 GPT와 같은 언어 모델을 저장하기 위한 파일 형식이다.
- GGML은 조지 게르가노프가 개발한 머신 러닝을 위해 설계된 텐서 라이브러리로, Apple Silicon을 비롯한 다양한 하드웨어에서 대규모 모델과 고성능을 구현할 수 있도록 지원한다.
- GGML은 GPT 모델을 위한 최초 파일형식이다.
- GGML은 여러 형식의 파일이 다수 존재한다. (즉 여러 파일을 다운로드 받아야 한다)
- GGUF는 하나의 파일로 작성되어 사용하기 용이하고 단일파일로 공유가 쉽다.

#### 모델 종류
- 명령(instruct) 모델
	- 주어진 명령이나 설명을 이해하고 답변을 잘 응답하도록 하는 모델
	- 예: gpt-4o, gemma-3, EXAONE-3.5-7.8B-Instruct-Q8_0
- 추론(reasoning) 모델
	- 논리적인 추론과 문제해결을 잘하는 모델
	- 프롬프트 입력 시 바로 답변토큰을 생성하지 않고 문제를 이해하고 어떤식으로 풀어야 할 지 생각하는 토큰을 생성시킨다. 그리고 나서 답변 토큰을 생성한다.
	- 예: gpt-o1, gpt-o3, deepseek-r1
- 증류(distill) 모델
	- 더 작은 모델로 압축하면서도 성능을 유지하는 모델
	- 예: DeepSeek-R1-Distill-Qwen-14B-Q8_0, DeepSeek-R1-Distill-Llama-8B-Q8_0


## 4. 템플릿 저장 (경로: models)
```bash
FROM EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf
TEMPLATE """{{- if .System }}
<s>{{ .System }}</s>
{{- end }}
<s>Human:
{{ .Prompt }}</s>
<s>Assistant:
"""

SYSTEM """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."""

PARAMETER stop <s>
PARAMETER stop </s>
```


## 5. 모델 생성

#### 해당 디렉토리로 이동
```bash
cd models
```

#### 모델파일 생성
```bash
ollama create EEVE-Korean-10.8B -f Modelfile
```

#### 생성결과 확인
```bash
ollama list

NAME                           ID              SIZE      MODIFIED       
EEVE-Korean-10.8B:latest       d6109bf62021    11 GB     12 seconds ago 
```

#### 실행
```bash
ollama run EEVE-Korean-10.8B:latest
```

#### 질문
```bash
> 대한민국의 수도는?
```


#### 빠져나갈때
```bash
> /bye
```


#### 09-Ollama.ipynb로 실행
```python
llm = ChatOllama(model="EEVE-Korean-10.8B:latest")
```





### gemma:7b
```bash
ollama pull gemma:7b
```

### llava:7b
```bash
ollama pull llava:7b
```


## deepseek-r1

```bash
ollama pull deepseek-r1:7b
```


## exaone 3.5 7.8b
```bash
ollama pull exaone3.5:7.8b
```

