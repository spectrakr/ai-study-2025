{
 "cells": [
  {
   "cell_type": "code",
   "id": "de9d9d8d",
   "metadata": {},
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_time():\n",
    "    \"\"\"Use this to get current date or time\"\"\"\n",
    "    return datetime.now()"
   ],
   "id": "b04c5d65512d5a62",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2230e22c",
   "metadata": {},
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 메모리 저장소 생성\n",
    "memory = MemorySaver()"
   ],
   "id": "bf4a2902371f65a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "tools = [get_current_time]\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm = llm.bind_tools(tools)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer in Korean.\",\n",
    "        ),\n",
    "        (\"human\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "graph = create_react_agent(llm, tools, state_modifier=prompt, checkpointer=memory)"
   ],
   "id": "da2665b609e953a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph, xray=True)"
   ],
   "id": "195a00ccecb8e2af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "question = \"한국의 수도는?\"\n",
    "inputs = {\"messages\": [(\"user\", question)], }\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10,  # 최대 10개의 노드까지 방문. 그 이상은 RecursionError 발생\n",
    "    configurable={\"thread_id\": \"1\"},  # 스레드 ID 설정\n",
    ")\n",
    "\n",
    "for chunk_msg, metadata  in graph.stream(inputs, stream_mode=\"messages\", config=config):\n",
    "    if (\n",
    "            chunk_msg.content\n",
    "            and not isinstance(chunk_msg, HumanMessage)\n",
    "            and metadata[\"langgraph_node\"] == \"agent\"\n",
    "    ):\n",
    "        print(chunk_msg.content, end=\"\", flush=True)"
   ],
   "id": "a126b48acd3759fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"그럼 일본은?\"\n",
    "inputs = {\"messages\": [(\"user\", question)], }\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10,  # 최대 10개의 노드까지 방문. 그 이상은 RecursionError 발생\n",
    "    configurable={\"thread_id\": \"1\"},  # 스레드 ID 설정\n",
    ")\n",
    "\n",
    "for chunk_msg, metadata  in graph.stream(inputs, stream_mode=\"messages\", config=config):\n",
    "    if (\n",
    "            chunk_msg.content\n",
    "            and not isinstance(chunk_msg, HumanMessage)\n",
    "            and metadata[\"langgraph_node\"] == \"agent\"\n",
    "    ):\n",
    "        print(chunk_msg.content, end=\"\", flush=True)"
   ],
   "id": "b0cf96bc4f113703",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    configurable={\"thread_id\": \"1\"},  # 스레드 ID 설정\n",
    ")\n",
    "# 그래프 상태 스냅샷 생성\n",
    "snapshot = graph.get_state(config)\n",
    "snapshot.values[\"messages\"]"
   ],
   "id": "63a35ec95aa8f655",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "question = \"현재 날짜는?\"\n",
    "inputs = {\"messages\": [(\"user\", question)], }\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10,  # 최대 10개의 노드까지 방문. 그 이상은 RecursionError 발생\n",
    "    configurable={\"thread_id\": \"2\"},  # 스레드 ID 설정\n",
    ")\n",
    "\n",
    "for chunk_msg, metadata  in graph.stream(inputs, stream_mode=\"messages\", config=config):\n",
    "    if (\n",
    "            chunk_msg.content\n",
    "            and not isinstance(chunk_msg, HumanMessage)\n",
    "            and metadata[\"langgraph_node\"] == \"agent\"\n",
    "    ):\n",
    "        print(chunk_msg.content, end=\"\", flush=True)"
   ],
   "id": "3f8396c8ee934563",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "question = \"그럼 어제는?\"\n",
    "inputs = {\"messages\": [(\"user\", question)], }\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10,  # 최대 10개의 노드까지 방문. 그 이상은 RecursionError 발생\n",
    "    configurable={\"thread_id\": \"2\"},  # 스레드 ID 설정\n",
    ")\n",
    "\n",
    "for chunk_msg, metadata  in graph.stream(inputs, stream_mode=\"messages\", config=config):\n",
    "    if (\n",
    "            chunk_msg.content\n",
    "            and not isinstance(chunk_msg, HumanMessage)\n",
    "            and metadata[\"langgraph_node\"] == \"agent\"\n",
    "    ):\n",
    "        print(chunk_msg.content, end=\"\", flush=True)"
   ],
   "id": "9c8bb6a7d3f922e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = RunnableConfig(\n",
    "    configurable={\"thread_id\": \"2\"},  # 스레드 ID 설정\n",
    ")\n",
    "# 그래프 상태 스냅샷 생성\n",
    "snapshot = graph.get_state(config)\n",
    "snapshot.values[\"messages\"]"
   ],
   "id": "498a74a577d5509c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "question = \"지금 몇일이야?\"\n",
    "inputs = {\"messages\": [(\"user\", question)]}\n",
    "for chunk_msg, metadata  in graph.stream(inputs, stream_mode=\"messages\"):\n",
    "    if (\n",
    "            chunk_msg.content\n",
    "            and not isinstance(chunk_msg, HumanMessage)\n",
    "            and metadata[\"langgraph_node\"] == \"agent\"\n",
    "    ):\n",
    "        print(chunk_msg.content, end=\"\", flush=True)"
   ],
   "id": "a9ace53b43be782f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "78b7c72bc98ab672",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-lwwSZlnu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
