{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 도구 (Tools)\n",
    "\n",
    "도구(Tool)는 에이전트, 체인 또는 LLM이 외부 세계와 상호작용하기 위한 인터페이스입니다.\n",
    "\n",
    "LangChain 에서 기본 제공하는 도구를 사용하여 쉽게 도구를 활용할 수 있으며, 사용자 정의 도구(Custom Tool) 를 쉽게 구축하는 것도 가능합니다.\n",
    "\n",
    "**LangChain 에 통합된 도구 리스트는 아래 링크에서 확인할 수 있습니다.**\n",
    "\n",
    "- [LangChain 통합된 도구 리스트](https://python.langchain.com/v0.1/docs/integrations/tools/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:37:29.809169Z",
     "start_time": "2025-03-25T04:37:29.799537Z"
    }
   },
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:38:28.066049Z",
     "start_time": "2025-03-25T04:38:28.062596Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빌트인 도구(built-in tools)\n",
    "\n",
    "랭체인에서 제공하는 사전에 정의된 도구(tool) 와 툴킷(toolkit) 을 사용할 수 있습니다.\n",
    "\n",
    "tool 은 단일 도구를 의미하며, toolkit 은 여러 도구를 묶어서 하나의 도구로 사용할 수 있습니다.\n",
    "\n",
    "관련 도구는 아래의 링크에서 참고하실 수 있습니다.\n",
    "\n",
    "**참고**\n",
    "- [LangChain Tools/Toolkits](https://python.langchain.com/docs/integrations/tools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python REPL 도구\n",
    "\n",
    "이 도구는 Python 코드를 REPL(Read-Eval-Print Loop) 환경에서 실행하기 위한 클래스를 제공합니다\n",
    "- PythonREPLTool\n",
    "\n",
    "**설명**\n",
    "\n",
    "- Python 셸 환경을 제공합니다.\n",
    "- 유효한 Python 명령어를 입력으로 받아 실행합니다.\n",
    "- 결과를 보려면 print(...) 함수를 사용해야 합니다.\n",
    "\n",
    "**주요 특징**\n",
    "\n",
    "- sanitize_input: 입력을 정제하는 옵션 (기본값: True)\n",
    "- python_repl: PythonREPL 인스턴스 (기본값: 전역 범위에서 실행)\n",
    "\n",
    "**사용 방법**\n",
    "\n",
    "- PythonREPLTool 인스턴스 생성\n",
    "- run 또는 arun, invoke 메서드를 사용하여 Python 코드 실행\n",
    "\n",
    "**입력 정제**\n",
    "\n",
    "- 입력 문자열에서 불필요한 공백, 백틱, 'python' 키워드 등을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:38:30.206814Z",
     "start_time": "2025-03-25T04:38:30.200788Z"
    }
   },
   "source": [
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# 파이썬 코드를 실행하는 도구를 생성합니다.\n",
    "python_tool = PythonREPLTool()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:39:23.648231Z",
     "start_time": "2025-03-25T04:39:23.644716Z"
    }
   },
   "source": [
    "# 파이썬 코드를 실행하고 결과를 반환합니다.\n",
    "print(python_tool.invoke(\"print(100 + 200)\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 LLM 에게 파이썬 코드를 작성하도록 요청하고 결과를 반환하는 예제입니다.\n",
    "\n",
    "**흐름 정리**\n",
    "1. LLM 모델에게 특정 작업을 수행하는 Python 코드를 작성하도록 요청합니다.\n",
    "2. 작성된 코드를 실행하여 결과를 얻습니다.\n",
    "3. 결과를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:41:27.053229Z",
     "start_time": "2025-03-25T04:41:27.035796Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# 파이썬 코드를 실행하고 중간 과정을 출력하고 도구 실행 결과를 반환하는 함수\n",
    "def print_and_execute(code, debug=False):\n",
    "    if debug:\n",
    "        print(\"CODE:\")\n",
    "        print(code)\n",
    "    return python_tool.invoke(code)\n",
    "\n",
    "\n",
    "# 파이썬 코드를 작성하도록 요청하는 프롬프트\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are Raymond Hetting, an expert python programmer, well versed in meta-programming and elegant, concise and short but well documented code. You follow the PEP8 style guide. \"\n",
    "            \"Return only the code, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the code.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "# LLM 모델 생성\n",
    "# gpt-4o-mini로 하면 결과가 나오지 않는 경우가 있다.\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 프롬프트와 LLM 모델을 사용하여 체인 생성\n",
    "chain = prompt | llm | StrOutputParser() | RunnableLambda(print_and_execute)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:41:29.297241Z",
     "start_time": "2025-03-25T04:41:28.561056Z"
    }
   },
   "source": [
    "# 결과 출력\n",
    "print(chain.invoke(\"로또 번호 생성기를 출력하는 코드를 작성하세요.\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import random\n",
      "\n",
      "def generate_lotto_numbers():\n",
      "    return sorted(random.sample(range(1, 46), 6))\n",
      "\n",
      "print(generate_lotto_numbers())\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색 API 도구\n",
    "\n",
    "Tavily 검색 API를 활용하여 검색 기능을 구현하는 도구입니다. 이 도구는 두 가지 주요 클래스를 제공합니다: `TavilySearchResults`와 `TavilyAnswer`.\n",
    "\n",
    "**API 키 발급 주소**\n",
    "- https://app.tavily.com/\n",
    "\n",
    "발급한 API 키를 환경변수에 설정합니다.\n",
    "\n",
    "`.env` 파일에 아래와 같이 설정합니다.\n",
    "\n",
    "```\n",
    "TAVILY_API_KEY=tvly-abcdefghijklmnopqrstuvwxyz\n",
    "```\n",
    "\n",
    "### TavilySearchResults\n",
    "\n",
    "**설명**\n",
    "- Tavily 검색 API를 쿼리하고 JSON 형식의 결과를 반환합니다.\n",
    "- 포괄적이고 정확하며 신뢰할 수 있는 결과에 최적화된 검색 엔진입니다.\n",
    "- 현재 이벤트에 대한 질문에 답변할 때 유용합니다.\n",
    "\n",
    "**주요 매개변수**\n",
    "- `max_results` (int): 반환할 최대 검색 결과 수 (기본값: 5)\n",
    "- `search_depth` (str): 검색 깊이 (\"basic\" 또는 \"advanced\"), advanced는 비용이 더 많이 청구될 수 있다.\n",
    "- `include_domains` (List[str]): 검색 결과에 포함할 도메인 목록\n",
    "- `exclude_domains` (List[str]): 검색 결과에서 제외할 도메인 목록\n",
    "- `include_answer` (bool): 원본 쿼리에 대한 짧은 답변 포함 여부\n",
    "- `include_raw_content` (bool): 각 사이트의 정제된 HTML 콘텐츠 포함 여부\n",
    "- `include_images` (bool): 쿼리 관련 이미지 목록 포함 여부\n",
    "\n",
    "**반환 값**\n",
    "- 검색 결과를 포함하는 JSON 형식의 문자열(url, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "혹은 아래의 주석을 해제하고 발급받은 API 키를 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"TAVILY API 키 입력\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:44:03.808204Z",
     "start_time": "2025-03-25T04:44:03.716034Z"
    }
   },
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# 도구 생성\n",
    "tool = TavilySearchResults(\n",
    "    max_results=3, # 최대 6개. 6개보다 적을 수 있다. (관련성이 없는 경우)\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    # include_images=True,\n",
    "    # search_depth=\"advanced\", # or \"basic\"\n",
    "    include_domains=[\"tistory.com\"],\n",
    "    # exclude_domains = []\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:44:18.665125Z",
     "start_time": "2025-03-25T04:44:10.326493Z"
    }
   },
   "source": [
    "# 도구 실행\n",
    "tool.invoke({\"query\": \"langgraph에서 RAG를 streaming 출력하는 방법\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '[langgraph] FastAPI와 LangGraph로 기본 RAG를 Streaming으로 출력하기',\n",
       "  'url': 'https://rudaks.tistory.com/entry/langgraph-FastAPI와-LangGraph로-기본-RAG를-Streaming으로-출력하기',\n",
       "  'content': '[langgraph] FastAPI와 LangGraph로 기본 RAG를 Streaming으로 출력하기\\n\\nFastAPI와 LangGraph를 이용하여 RAG의 응답을 Streaming으로 출력하는 방법을 알아보자.\\n\\n일단 검색 및 생성을 하기 위해서는 RAG pipeline을 구성을 해야 하는데 아주 간단한 방법으로만 할 예정이다.\\n\\n우선 아래의 방법으로 진행할 예정이다.\\n\\n작업 순서\\n\\n1. 환경설정\\n\\n.env 설정\\n\\n.env에 OPENAI_API_KEY를 추가한다.\\n\\n패키지 설치\\n\\n아래 예제에서 사용하는 패키지를 설치한다.\\n\\n2. langgraph 생성\\n\\n우선 RAG 파이프라인을 만들어 볼 예정이다. 아래와 같이 가장 단순한 방식으로 구성한다.\\n\\n\\n\\n임베딩\\n\\n우선 특정 데이터를 chromadb에 임베딩을 한다. 예제에서는 야구규칙이 있는 namu wiki의 내용을 WebBaseLoader를 사용하여 저장한다.\\n\\n[ingestion.py] [...] /search로 query라는 검색어를 파라미터로 받으며 graph_app을 호출한 결과를 SSE 방식으로 사용할 수 있게 stream으로 리턴한다.\\n\\n여기서 stream_mode를 custom으로 해줘야 실제 답변의 내용만을 받을 수 있다. stream_mode에는 updates, values 등 다른 값이 있는데, 답변 외에 변경된 값 혹은 state에 있는 모든 값을 받고 싶다면 옵션을 조정할 수도 있다.\\n\\n위에서 chunk[1]은 응답으로 chunk의 내용이 tuple로 넘어온다. 그래서 두번째 값이 실제 응답이라 추가하였다.\\n\\nchunk 내용\\n\\nStreamingResponse는 일반적인 streaming으로 응답을 보낼 때 사용하는 방식이다.\\n\\n4. 테스트\\n\\n우선 테스트를 위해서 fastapi를 시작하자.\\n\\napi 실행\\n\\n실행결과\\n\\n참고\\n\\n댓글을 달아 주세요\\n\\n전체 방문자\\n\\n전체 카테고리\\n\\n최근 글\\n\\n최근댓글\\n\\n블로그 인기글\\n\\n티스토리툴바 [...] 위에서 StreamWriter는 각 chunk를 stream으로 출력하기 위해 필요하다. StreamWriter에 각 chunk를 넣어줘야 출력이 된다.\\n\\n그리고 state에 chunk의 완성체를 추가하여 다음 노드에서 사용할 수 있게 한다.\\n\\n다음으로 chain을 생성한다.\\n\\n[generation.py]\\n\\nrag-prompt는 langchain 팀에서 만든 가장 많이 사용하는 rag 프롬프트이다.\\n\\n마지막으로 graph를 생성한다.\\n\\n[graph.py]\\n\\n2개의 노드를 만들고 각 edge를 연결한다.\\n\\nstart -> retrieveretrieve -> generategenerate -> end\\n\\ndraw_mermaid_png를 실행하면 graph.png로 이미지가 생성되는 것을 확인할 수 있다.\\n\\n3. Fastapi 생성\\n\\n이제 api를 만들어보자.',\n",
       "  'score': 0.92700076,\n",
       "  'raw_content': '[langgraph] FastAPI와 LangGraph로 기본 RAG를 Streaming으로 출력하기\\n\\nFastAPI와 LangGraph를 이용하여 RAG의 응답을 Streaming으로 출력하는 방법을 알아보자.\\n\\n일단 검색 및 생성을 하기 위해서는 RAG pipeline을 구성을 해야 하는데 아주 간단한 방법으로만 할 예정이다.\\n\\n우선 아래의 방법으로 진행할 예정이다.\\n\\n작업 순서\\n\\n1. 환경설정\\n\\n.env 설정\\n\\n.env에 OPENAI_API_KEY를 추가한다.\\n\\n패키지 설치\\n\\n아래 예제에서 사용하는 패키지를 설치한다.\\n\\n2. langgraph 생성\\n\\n우선 RAG 파이프라인을 만들어 볼 예정이다. 아래와 같이 가장 단순한 방식으로 구성한다.\\n\\n\\n\\n임베딩\\n\\n우선 특정 데이터를 chromadb에 임베딩을 한다. 예제에서는 야구규칙이 있는 namu wiki의 내용을 WebBaseLoader를 사용하여 저장한다.\\n\\n[ingestion.py]\\n\\n그리고 위의 파일을 실행을 해보자. 그러면 로컬에 chroma 파일이 생성이 되고 url 데이터가 저장되는 것을 확인할 수 있다.\\n\\n이 파일에서 retriever를 다른 파일에서 읽어올 예정인데 Chroma.from_documents 부분을 주석처리한다. 그렇지 않으면 매번 vector db에 문서가 저장이 되어 중복데이터가 생긴다.\\n\\ngraph 생성\\n\\nlanggraph를 사용하기 위해서는 state, node, chain을 만들어야 한다.\\n\\n우선 state를 만든다.\\n\\n[state.py]\\n\\nquestion은 사용자의 prompt 내용이고 generation은 LLM에서 생성된 completion 데이터이다. documents는 vectordb에서 검색한 결과이다.\\n\\n그리고 retrieve, generate의 두 개의 node를 만든다.\\n\\n[retrieve.py]\\n\\n[generate.py]\\n\\napi에서 호출방식을 async로 사용할 예정이므로 함수는 async로 지정한다.\\n\\n위에서 StreamWriter는 각 chunk를 stream으로 출력하기 위해 필요하다. StreamWriter에 각 chunk를 넣어줘야 출력이 된다.\\n\\n그리고 state에 chunk의 완성체를 추가하여 다음 노드에서 사용할 수 있게 한다.\\n\\n다음으로 chain을 생성한다.\\n\\n[generation.py]\\n\\nrag-prompt는 langchain 팀에서 만든 가장 많이 사용하는 rag 프롬프트이다.\\n\\n마지막으로 graph를 생성한다.\\n\\n[graph.py]\\n\\n2개의 노드를 만들고 각 edge를 연결한다.\\n\\nstart -> retrieveretrieve -> generategenerate -> end\\n\\ndraw_mermaid_png를 실행하면 graph.png로 이미지가 생성되는 것을 확인할 수 있다.\\n\\n3. Fastapi 생성\\n\\n이제 api를 만들어보자.\\n\\n/search로 query라는 검색어를 파라미터로 받으며 graph_app을 호출한 결과를 SSE 방식으로 사용할 수 있게 stream으로 리턴한다.\\n\\n여기서 stream_mode를 custom으로 해줘야 실제 답변의 내용만을 받을 수 있다. stream_mode에는 updates, values 등 다른 값이 있는데, 답변 외에 변경된 값 혹은 state에 있는 모든 값을 받고 싶다면 옵션을 조정할 수도 있다.\\n\\n위에서 chunk[1]은 응답으로 chunk의 내용이 tuple로 넘어온다. 그래서 두번째 값이 실제 응답이라 추가하였다.\\n\\nchunk 내용\\n\\nStreamingResponse는 일반적인 streaming으로 응답을 보낼 때 사용하는 방식이다.\\n\\n4. 테스트\\n\\n우선 테스트를 위해서 fastapi를 시작하자.\\n\\napi 실행\\n\\n실행결과\\n\\n참고\\n\\n댓글을 달아 주세요\\n\\n전체 방문자\\n\\n전체 카테고리\\n\\n최근 글\\n\\n최근댓글\\n\\n블로그 인기글\\n\\n티스토리툴바\\n\\n\\n\\n'},\n",
       " {'title': '[langgraph] FastAPI와 LangGraph로 기본 RAG를 Streaming ...',\n",
       "  'url': 'https://rudaks.tistory.com/entry/langgraph-FastAPI%EC%99%80-LangGraph%EB%A1%9C-%EA%B8%B0%EB%B3%B8-RAG%EB%A5%BC-Streaming%EC%9C%BC%EB%A1%9C-%EC%B6%9C%EB%A0%A5%ED%95%98%EA%B8%B0',\n",
       "  'content': '[langgraph] FastAPI와 LangGraph로 기본 RAG를 Streaming으로 출력하기\\n\\nFastAPI와 LangGraph를 이용하여 RAG의 응답을 Streaming으로 출력하는 방법을 알아보자.\\n\\n일단 검색 및 생성을 하기 위해서는 RAG pipeline을 구성을 해야 하는데 아주 간단한 방법으로만 할 예정이다.\\n\\n우선 아래의 방법으로 진행할 예정이다.\\n\\n작업 순서\\n\\n1. 환경설정\\n\\n.env 설정\\n\\n.env에 OPENAI_API_KEY를 추가한다.\\n\\n패키지 설치\\n\\n아래 예제에서 사용하는 패키지를 설치한다.\\n\\n2. langgraph 생성\\n\\n우선 RAG 파이프라인을 만들어 볼 예정이다. 아래와 같이 가장 단순한 방식으로 구성한다.\\n\\n\\n\\n임베딩\\n\\n우선 특정 데이터를 chromadb에 임베딩을 한다. 예제에서는 야구규칙이 있는 namu wiki의 내용을 WebBaseLoader를 사용하여 저장한다.\\n\\n[ingestion.py] [...] 위에서 StreamWriter는 각 chunk를 stream으로 출력하기 위해 필요하다. StreamWriter에 각 chunk를 넣어줘야 출력이 된다.\\n\\n그리고 state에 chunk의 완성체를 추가하여 다음 노드에서 사용할 수 있게 한다.\\n\\n다음으로 chain을 생성한다.\\n\\n[generation.py]\\n\\nrag-prompt는 langchain 팀에서 만든 가장 많이 사용하는 rag 프롬프트이다.\\n\\n마지막으로 graph를 생성한다.\\n\\n[graph.py]\\n\\n2개의 노드를 만들고 각 edge를 연결한다.\\n\\nstart -> retrieveretrieve -> generategenerate -> end\\n\\ndraw_mermaid_png를 실행하면 graph.png로 이미지가 생성되는 것을 확인할 수 있다.\\n\\n3. Fastapi 생성\\n\\n이제 api를 만들어보자. [...] /search로 query라는 검색어를 파라미터로 받으며 graph_app을 호출한 결과를 SSE 방식으로 사용할 수 있게 stream으로 리턴한다.\\n\\n여기서 stream_mode를 custom으로 해줘야 실제 답변의 내용만을 받을 수 있다. stream_mode에는 updates, values 등 다른 값이 있는데, 답변 외에 변경된 값 혹은 state에 있는 모든 값을 받고 싶다면 옵션을 조정할 수도 있다.\\n\\n위에서 chunk[1]은 응답으로 chunk의 내용이 tuple로 넘어온다. 그래서 두번째 값이 실제 응답이라 추가하였다.\\n\\nchunk 내용\\n\\nStreamingResponse는 일반적인 streaming으로 응답을 보낼 때 사용하는 방식이다.\\n\\n4. 테스트\\n\\n우선 테스트를 위해서 fastapi를 시작하자.\\n\\napi 실행\\n\\n실행결과\\n\\n참고\\n\\n댓글을 달아 주세요\\n\\n전체 방문자\\n\\n전체 카테고리\\n\\n최근 글\\n\\n최근댓글\\n\\n블로그 인기글\\n\\n티스토리툴바',\n",
       "  'score': 0.9257218,\n",
       "  'raw_content': '[langgraph] FastAPI와 LangGraph로 기본 RAG를 Streaming으로 출력하기\\n\\nFastAPI와 LangGraph를 이용하여 RAG의 응답을 Streaming으로 출력하는 방법을 알아보자.\\n\\n일단 검색 및 생성을 하기 위해서는 RAG pipeline을 구성을 해야 하는데 아주 간단한 방법으로만 할 예정이다.\\n\\n우선 아래의 방법으로 진행할 예정이다.\\n\\n작업 순서\\n\\n1. 환경설정\\n\\n.env 설정\\n\\n.env에 OPENAI_API_KEY를 추가한다.\\n\\n패키지 설치\\n\\n아래 예제에서 사용하는 패키지를 설치한다.\\n\\n2. langgraph 생성\\n\\n우선 RAG 파이프라인을 만들어 볼 예정이다. 아래와 같이 가장 단순한 방식으로 구성한다.\\n\\n\\n\\n임베딩\\n\\n우선 특정 데이터를 chromadb에 임베딩을 한다. 예제에서는 야구규칙이 있는 namu wiki의 내용을 WebBaseLoader를 사용하여 저장한다.\\n\\n[ingestion.py]\\n\\n그리고 위의 파일을 실행을 해보자. 그러면 로컬에 chroma 파일이 생성이 되고 url 데이터가 저장되는 것을 확인할 수 있다.\\n\\n이 파일에서 retriever를 다른 파일에서 읽어올 예정인데 Chroma.from_documents 부분을 주석처리한다. 그렇지 않으면 매번 vector db에 문서가 저장이 되어 중복데이터가 생긴다.\\n\\ngraph 생성\\n\\nlanggraph를 사용하기 위해서는 state, node, chain을 만들어야 한다.\\n\\n우선 state를 만든다.\\n\\n[state.py]\\n\\nquestion은 사용자의 prompt 내용이고 generation은 LLM에서 생성된 completion 데이터이다. documents는 vectordb에서 검색한 결과이다.\\n\\n그리고 retrieve, generate의 두 개의 node를 만든다.\\n\\n[retrieve.py]\\n\\n[generate.py]\\n\\napi에서 호출방식을 async로 사용할 예정이므로 함수는 async로 지정한다.\\n\\n위에서 StreamWriter는 각 chunk를 stream으로 출력하기 위해 필요하다. StreamWriter에 각 chunk를 넣어줘야 출력이 된다.\\n\\n그리고 state에 chunk의 완성체를 추가하여 다음 노드에서 사용할 수 있게 한다.\\n\\n다음으로 chain을 생성한다.\\n\\n[generation.py]\\n\\nrag-prompt는 langchain 팀에서 만든 가장 많이 사용하는 rag 프롬프트이다.\\n\\n마지막으로 graph를 생성한다.\\n\\n[graph.py]\\n\\n2개의 노드를 만들고 각 edge를 연결한다.\\n\\nstart -> retrieveretrieve -> generategenerate -> end\\n\\ndraw_mermaid_png를 실행하면 graph.png로 이미지가 생성되는 것을 확인할 수 있다.\\n\\n3. Fastapi 생성\\n\\n이제 api를 만들어보자.\\n\\n/search로 query라는 검색어를 파라미터로 받으며 graph_app을 호출한 결과를 SSE 방식으로 사용할 수 있게 stream으로 리턴한다.\\n\\n여기서 stream_mode를 custom으로 해줘야 실제 답변의 내용만을 받을 수 있다. stream_mode에는 updates, values 등 다른 값이 있는데, 답변 외에 변경된 값 혹은 state에 있는 모든 값을 받고 싶다면 옵션을 조정할 수도 있다.\\n\\n위에서 chunk[1]은 응답으로 chunk의 내용이 tuple로 넘어온다. 그래서 두번째 값이 실제 응답이라 추가하였다.\\n\\nchunk 내용\\n\\nStreamingResponse는 일반적인 streaming으로 응답을 보낼 때 사용하는 방식이다.\\n\\n4. 테스트\\n\\n우선 테스트를 위해서 fastapi를 시작하자.\\n\\napi 실행\\n\\n실행결과\\n\\n참고\\n\\n댓글을 달아 주세요\\n\\n전체 방문자\\n\\n전체 카테고리\\n\\n최근 글\\n\\n최근댓글\\n\\n블로그 인기글\\n\\n티스토리툴바\\n\\n\\n\\n'},\n",
       " {'title': '[langgraph] Persistence - [루닥스 블로그] 연습만이 살길이다',\n",
       "  'url': 'https://rudaks.tistory.com/entry/langgraph-Persistence',\n",
       "  'content': '그래서 LangGraph가 제공하는 방법은 그래프 실행을 멈추는 것이다. ... [langgraph] FastAPI와 LangGraph로 기본 RAG를 Streaming으로 출력하기.',\n",
       "  'score': 0.8655088}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 생성 도구 (DALL-E)\n",
    "\n",
    "- `DallEAPIWrapper 클래스`: OpenAI의 DALL-E 이미지 생성기를 위한 래퍼(wrapper)입니다.\n",
    "\n",
    "이 도구를 사용하면 DALL-E API를 쉽게 통합하여 텍스트 기반 이미지 생성 기능을 구현할 수 있습니다. 다양한 설정 옵션을 통해 유연하고 강력한 이미지 생성 도구로 활용할 수 있습니다.\n",
    "\n",
    "**주요 속성**\n",
    "\n",
    "- `model`: 사용할 DALL-E 모델 이름 (기본값: \"dall-e-2\", \"dall-e-3\")\n",
    "\n",
    "- `n`: 생성할 이미지 수 (기본값: 1)\n",
    "\n",
    "- `size`: 생성할 이미지 크기\n",
    "  - \"dall-e-2\": \"1024x1024\", \"512x512\", \"256x256\"\n",
    "  - \"dall-e-3\": \"1024x1024\", \"1792x1024\", \"1024x1792\"\n",
    "\n",
    "- `style`: 생성될 이미지의 스타일 (기본값: \"natural\", \"vivid\")\n",
    "\n",
    "- `quality`: 생성될 이미지의 품질 (기본값: \"standard\", \"hd\")\n",
    "\n",
    "- `max_retries`: 생성 시 최대 재시도 횟수\n",
    "\n",
    "**주요 기능**\n",
    "- DALL-E API를 사용하여 텍스트 설명에 기반한 이미지 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**흐름 정리**\n",
    "\n",
    "다음은 DALL-E Image Generator 를 사용하여 이미지를 생성하는 예제입니다.\n",
    "\n",
    "이번에는 `DallEAPIWrapper` 를 사용하여 이미지를 생성해 보겠습니다.\n",
    "\n",
    "이때 입력 프롬프트는 LLM 모델에게 이미지를 생성하는 프롬프트를 작성하도록 요청합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:46:55.626571Z",
     "start_time": "2025-03-25T04:46:53.324376Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI 모델 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.9, max_tokens=1000)\n",
    "\n",
    "# DALL-E 이미지 생성을 위한 프롬프트 템플릿 정의\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Generate a detailed IMAGE GENERATION prompt for DALL-E based on the following description. \"\n",
    "    \"Return only the prompt, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the prompt\"\n",
    "    \"Output should be less than 1000 characters. Write in English only.\"\n",
    "    \"Image Description: \\n{image_desc}\",\n",
    ")\n",
    "\n",
    "# 프롬프트, LLM, 출력 파서를 연결하는 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 체인 실행\n",
    "image_prompt = chain.invoke(\n",
    "    # {\"image_desc\": \"스마트폰을 바라보는 사람들을 풍자한 neo-classicism painting\"}\n",
    "    {\"image_desc\": \"프로야구에서 빈볼시비가 벌어지는 장면\"}\n",
    ")\n",
    "\n",
    "# 이미지 프롬프트 출력\n",
    "print(image_prompt)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dynamic baseball scene capturing the moment when the Samsung Lions achieve a decisive victory over the KT Wiz in a professional baseball game. The stadium is filled with enthusiastic fans, waving blue and white flags representing the Samsung Lions, while a sea of purple and gold from the KT Wiz fans is visible in the background. The players are in action, with a Samsung Lions batter hitting a home run, the ball soaring through the air. The scoreboard in the background shows a significant lead for the Lions, and confetti is falling from above. The atmosphere is electric, showcasing the excitement and energy of the game, with bright stadium lights illuminating the field. Make sure to include details like the players' jerseys, the intricate design of the stadium, and the vibrant crowd reaction.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼, 이전에 생성한 이미지 프롬프트를 `DallEAPIWrapper` 에 입력하여 이미지를 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DallEAPIWrapper` 에 대한 임시 버그 안내사항** (작성일: 2024-10-13)\n",
    "\n",
    "- 현재 langchain 0.3.x 이상 버전에서 `DallEAPIWrapper` 에 대한 임시 버그가 있습니다. (`401 오류: invalid API key`)\n",
    "\n",
    "따라서, 아래의 코드를 오류 없이 실행하기 위해서는 LangChain 버전을 0.2.16 으로 변경해야 합니다.\n",
    "\n",
    "아래의 주석을 해제하고 실행하면 LangChain 버전을 0.2.16 으로 변경됩니다.\n",
    "\n",
    "하지만, 이후 내용에서는 LangChain 버전을 0.3.x 이상으로 변경하여 사용하기 때문에\n",
    "\n",
    "`poetry shell` 명령어를 통해 다시 최신 langchain 버전으로 변경해야 합니다.\n",
    "\n",
    "이 과정이 번거로운 분들은 일단 `DallEAPIWrapper` 를 사용하지 않고 진행하셔도 무방합니다.\n",
    "\n",
    "**업그레이드/다운그레이드** 후에는 반드시 상단 메뉴의 \"Restart\" 버튼을 클릭한 뒤 진행해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 임시 버전 다운그레이드 명령어 (실행 후 restart)\n",
    "# !pip install langchain==0.2.16 langchain-community==0.2.16 langchain-text-splitters==0.2.4 langchain-experimental==0.0.65 langchain-openai==0.1.20"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:48:34.761087Z",
     "start_time": "2025-03-25T04:48:16.941240Z"
    }
   },
   "source": [
    "# DALL-E API 래퍼 가져오기\n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from IPython.display import Image\n",
    "import os\n",
    "\n",
    "# DALL-E API 래퍼 초기화\n",
    "# model: 사용할 DALL-E 모델 버전\n",
    "# size: 생성할 이미지 크기\n",
    "# quality: 이미지 품질\n",
    "# n: 생성할 이미지 수\n",
    "dalle = DallEAPIWrapper(\n",
    "    model=\"dall-e-3\",\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "# 질문\n",
    "# query = \"스마트폰을 바라보는 사람들을 풍자한 neo-classicism painting\"\n",
    "query = \"프로야구에서 빈볼시비가 벌어지는 장면\"\n",
    "\n",
    "# 이미지 생성 및 URL 받기\n",
    "# chain.invoke()를 사용하여 이미지 설명을 DALL-E 프롬프트로 변환\n",
    "# dalle.run()을 사용하여 실제 이미지 생성\n",
    "image_url = dalle.run(chain.invoke({\"image_desc\": query}))\n",
    "\n",
    "# 생성된 이미지를 표시합니다.\n",
    "Image(url=image_url, width=500)"
   ],
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-2cj1mRTuY3MUk3cwhouETZPp/user-pSJFv9fBvlFEqYapjOwoWZlG/img-5fIuex58URWi4wKJvZtaWrsu.png?st=2025-03-25T03%3A48%3A36Z&se=2025-03-25T05%3A48%3A36Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-03-24T07%3A18%3A39Z&ske=2025-03-25T07%3A18%3A39Z&sks=b&skv=2024-08-04&sig=hDT2%2BeljIQov6Ulnkw%2BukqD7fe7bKEJvkr/QsC2N/X8%3D\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 정의 도구(Custom Tool)\n",
    "\n",
    "LangChain 에서 제공하는 빌트인 도구 외에도 사용자가 직접 도구를 정의하여 사용할 수 있습니다.\n",
    "\n",
    "이를 위해서는 `langchain.tools` 모듈에서 제공하는 `tool` 데코레이터를 사용하여 함수를 도구로 변환합니다.\n",
    "\n",
    "### @tool 데코레이터\n",
    "\n",
    "이 데코레이터는 함수를 도구로 변환하는 기능을 제공합니다. 다양한 옵션을 통해 도구의 동작을 커스터마이즈할 수 있습니다.\n",
    "\n",
    "**사용 방법**\n",
    "1. 함수 위에 `@tool` 데코레이터 적용\n",
    "2. 필요에 따라 데코레이터 매개변수 설정\n",
    "\n",
    "이 데코레이터를 사용하면 일반 Python 함수를 강력한 도구로 쉽게 변환할 수 있으며, 자동화된 문서화와 유연한 인터페이스 생성이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "# 데코레이터를 사용하여 함수를 도구로 변환합니다.\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 도구 실행\n",
    "add_numbers.invoke({\"a\": 3, \"b\": 4})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 도구 실행\n",
    "multiply_numbers.invoke({\"a\": 3, \"b\": 4})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구글 뉴스기사 검색 도구\n",
    "\n",
    "`langchain-teddynote` 패키지에서 제공하는 `GoogleNews` 도구를 사용하여 구글 뉴스기사를 검색하는 도구입니다.\n",
    "\n",
    "**참고**\n",
    "- API 키가 필요하지 않습니다. (RSS 피드를 사용하기 때문)\n",
    "\n",
    "news.google.com 에서 제공하는 뉴스기사를 검색하는 도구입니다.\n",
    "\n",
    "**설명**\n",
    "- 구글 뉴스 검색 API를 사용하여 최신 뉴스를 검색합니다.\n",
    "- 키워드를 기반으로 뉴스를 검색할 수 있습니다.\n",
    "- 최신 뉴스를 검색할 수 있습니다.\n",
    "\n",
    "**주요 매개변수**\n",
    "- `k` (int): 반환할 최대 검색 결과 수 (기본값: 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용하기 전 패키지를 업데이트 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !pip install -qU langchain-teddynote"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_teddynote.tools import GoogleNews\n",
    "\n",
    "# 도구 생성\n",
    "news_tool = GoogleNews()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 최신 뉴스 검색\n",
    "news_tool.search_latest(k=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 키워드로 뉴스 검색\n",
    "news_tool.search_by_keyword(\"AI 투자\", k=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_teddynote.tools import GoogleNews\n",
    "from langchain.tools import tool\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "# 키워드로 뉴스 검색하는 도구 생성\n",
    "@tool\n",
    "def search_keyword(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Look up news by keyword\"\"\"\n",
    "    print(query)\n",
    "    news_tool = GoogleNews()\n",
    "    return news_tool.search_by_keyword(query, k=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 실행 결과\n",
    "search_keyword.invoke({\"query\": \"LangChain AI\"})"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-lwwSZlnu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
